import boto3
import json
import gzip

# Hardcoded S3 bucket name
S3_BUCKET_NAME = "your-cloudtrail-log-bucket"

s3 = boto3.client("s3")
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("IAMThreatLogs")

def lambda_handler(event, context):
    print("Received event:", json.dumps(event, indent=4))

    try:
        # Use the hardcoded S3 bucket name
        if "Records" in event and len(event["Records"]) > 0 and "s3" in event["Records"][0]:
            bucket_name = event["Records"][0]["s3"]["bucket"]["name"]
            object_key = event["Records"][0]["s3"]["object"]["key"]
        else:
            # If the event does not contain S3 details, use the hardcoded bucket name
            bucket_name = S3_BUCKET_NAME
            object_key = "path/to/cloudtrail/log.json.gz"  # Provide a sample log file path
        
        response = s3.get_object(Bucket=bucket_name, Key=object_key)
        log_data = gzip.decompress(response["Body"].read()).decode("utf-8")
        cloudtrail_events = json.loads(log_data)["Records"]

        iam_events = [
            event for event in cloudtrail_events if "iam.amazonaws.com" in event["eventSource"]
        ]

        for log in iam_events:
            table.put_item(Item=log)

        return {"status": "IAM logs stored in DynamoDB"}

    except Exception as e:
        print(f"Error processing event: {e}")
        return {"status": "Error processing event", "error": str(e)}